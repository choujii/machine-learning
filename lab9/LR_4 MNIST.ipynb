{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca67e86",
   "metadata": {},
   "source": [
    "# Лабораторная работа №4 — Классификация MNIST (MLP sklearn vs CNN LeNet (PyTorch))\n",
    "\n",
    "**ТЗ:**  \n",
    "1) Решить задачу классификации датасета **MNIST** с помощью **MLP из scikit-learn**.  \n",
    "2) Решить задачу классификации MNIST с помощью **CNN типа LeNet** в **PyTorch**.  \n",
    "3) Сравнить результаты по метрикам и сделать обоснованные выводы.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c33876",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b06fb",
   "metadata": {},
   "source": [
    "## 1) Загрузка MNIST\n",
    "\n",
    "Для sklearn-MLP используем `fetch_openml` .\n",
    "Для PyTorch-CNN используем `torchvision.datasets.MNIST`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d380be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), (60000,), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist.data.astype(np.float32) / 255.0\n",
    "y = mnist.target.astype(np.int64)\n",
    "\n",
    "# стандартное разбиение MNIST: 60k train, 10k test\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c3f9a",
   "metadata": {},
   "source": [
    "### Заключение по подготовке датасета MNIST\n",
    "\n",
    "В результате загрузки датасета MNIST была получена обучающая выборка размером 60000 изображений и тестовая выборка размером 10000 изображений. Каждое изображение представлено в виде вектора размерности 784, соответствующего изображению 28×28 пикселей.\n",
    "\n",
    "Целевые метки представлены в виде одномерного массива и соответствуют десяти классам цифр от 0 до 9. Такое представление данных является стандартным для задач классификации MNIST и корректно подходит как для MLP, так и для сверточных нейронных сетей.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca375a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x250 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Визуализация примеров\n",
    "fig, axes = plt.subplots(1, 6, figsize=(12, 2.5))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap=\"gray\")\n",
    "    ax.set_title(str(y_train[i]))\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e671879",
   "metadata": {},
   "source": [
    "### Заключение по визуализации данных\n",
    "\n",
    "Визуализация примеров изображений из обучающей выборки подтверждает корректность загрузки датасета MNIST. Изображения цифр хорошо различимы, классы соответствуют ожидаемым значениям, что указывает на отсутствие ошибок в процессе загрузки и предобработки данных.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57771da1",
   "metadata": {},
   "source": [
    "## 2) MLP (scikit-learn)\n",
    "\n",
    "Используем `MLPClassifier`. Для MNIST важно:\n",
    "- нормализация/стандартизация входа (добавим `StandardScaler`);\n",
    "- `early_stopping=True` чтобы не переобучаться и не ждать слишком долго;\n",
    "- разумная архитектура (256, 128) как baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b37dcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.29334084\n",
      "Validation score: 0.956500\n",
      "Iteration 2, loss = 0.09863697\n",
      "Validation score: 0.962000\n",
      "Iteration 3, loss = 0.05884222\n",
      "Validation score: 0.966833\n",
      "Iteration 4, loss = 0.03877557\n",
      "Validation score: 0.965667\n",
      "Iteration 5, loss = 0.02508578\n",
      "Validation score: 0.967000\n",
      "Iteration 6, loss = 0.01663570\n",
      "Validation score: 0.969667\n",
      "Iteration 7, loss = 0.01200930\n",
      "Validation score: 0.972333\n",
      "Iteration 8, loss = 0.01389391\n",
      "Validation score: 0.970667\n",
      "Iteration 9, loss = 0.01434095\n",
      "Validation score: 0.967333\n",
      "Iteration 10, loss = 0.01412294\n",
      "Validation score: 0.967833\n",
      "Iteration 11, loss = 0.02137252\n",
      "Validation score: 0.965833\n",
      "Validation score did not improve more than tol=0.000100 for 3 consecutive epochs. Stopping.\n",
      "MLP (sklearn) metrics:\n",
      "Accuracy: 0.9759\n",
      "Precision_macro: 0.9757746349708085\n",
      "Recall_macro: 0.9756299532723215\n",
      "F1_macro: 0.9756850122423062\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "mlp = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", MLPClassifier(\n",
    "        hidden_layer_sizes=(256, 128),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        batch_size=256,\n",
    "        learning_rate_init=1e-3,\n",
    "        max_iter=20,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=3,\n",
    "        verbose=True,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "acc_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "prec_mlp, rec_mlp, f1_mlp, _ = precision_recall_fscore_support(y_test, y_pred_mlp, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"MLP (sklearn) metrics:\")\n",
    "print(\"Accuracy:\", acc_mlp)\n",
    "print(\"Precision_macro:\", prec_mlp)\n",
    "print(\"Recall_macro:\", rec_mlp)\n",
    "print(\"F1_macro:\", f1_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ec9330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (MLP):\n",
      " [[ 969    0    1    0    0    2    2    2    3    1]\n",
      " [   0 1121    4    2    0    1    3    1    2    1]\n",
      " [   8    0 1007    1    2    1    5    5    3    0]\n",
      " [   0    0    8  981    1    7    2    3    3    5]\n",
      " [   1    0    5    3  954    0    4    3    2   10]\n",
      " [   4    0    0    9    2  859    6    1    7    4]\n",
      " [   5    3    2    1    3    2  939    0    3    0]\n",
      " [   1    3    7    2    0    0    0 1004    1   10]\n",
      " [   4    0    3    8    3    4    0    5  945    2]\n",
      " [   2    2    0    5    5    2    0    9    4  980]]\n",
      "\n",
      "Classification report (MLP):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9748    0.9888    0.9818       980\n",
      "           1     0.9929    0.9877    0.9903      1135\n",
      "           2     0.9711    0.9758    0.9734      1032\n",
      "           3     0.9694    0.9713    0.9703      1010\n",
      "           4     0.9835    0.9715    0.9775       982\n",
      "           5     0.9784    0.9630    0.9706       892\n",
      "           6     0.9771    0.9802    0.9786       958\n",
      "           7     0.9719    0.9767    0.9743      1028\n",
      "           8     0.9712    0.9702    0.9707       974\n",
      "           9     0.9674    0.9713    0.9693      1009\n",
      "\n",
      "    accuracy                         0.9759     10000\n",
      "   macro avg     0.9758    0.9756    0.9757     10000\n",
      "weighted avg     0.9759    0.9759    0.9759     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Матрица ошибок и отчет (MLP)\n",
    "cm_mlp = confusion_matrix(y_test, y_pred_mlp)\n",
    "print(\"Confusion matrix (MLP):\\n\", cm_mlp)\n",
    "\n",
    "print(\"\\nClassification report (MLP):\")\n",
    "print(classification_report(y_test, y_pred_mlp, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd9a79",
   "metadata": {},
   "source": [
    "### Заключение по обучению MLP (scikit-learn)\n",
    "\n",
    "В процессе обучения многослойного перцептрона наблюдалось устойчивое снижение значения функции потерь и рост значения validation score. Механизм ранней остановки (early stopping) сработал корректно, завершив обучение после отсутствия улучшения качества на валидационной выборке в течение нескольких эпох.\n",
    "\n",
    "Это свидетельствует о том, что модель достигла оптимального уровня обобщающей способности и не переобучилась.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a7cb5",
   "metadata": {},
   "source": [
    "## 3) CNN LeNet (PyTorch)\n",
    "\n",
    "Архитектура LeNet (адаптация под 28×28):\n",
    "- Conv(1→6, 5×5) → ReLU → AvgPool(2×2)\n",
    "- Conv(6→16, 5×5) → ReLU → AvgPool(2×2)\n",
    "- FC(16·4·4→120) → ReLU → FC(120→84) → ReLU → FC(84→10)\n",
    "\n",
    "Обучение: CrossEntropyLoss + Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94e4a23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a785e67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(469, 40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "len(train_loader), len(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22e48c",
   "metadata": {},
   "source": [
    "### Заключение по архитектуре CNN (LeNet)\n",
    "\n",
    "Используемая сверточная нейронная сеть основана на архитектуре LeNet и включает два сверточных слоя с последующим pooling, а также три полносвязных слоя. Такая архитектура позволяет эффективно извлекать локальные признаки изображений, такие как штрихи и контуры, что особенно важно для задач распознавания рукописных цифр.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01aeebd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)   # 28->24\n",
    "        self.pool  = nn.AvgPool2d(2, 2)               # 24->12\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)  # 12->8\n",
    "        # pool: 8->4\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "cnn = LeNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=1e-3)\n",
    "\n",
    "cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0bd009c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\0potter0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | loss=0.4902 | test_acc=0.9489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\0potter0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | loss=0.1380 | test_acc=0.9732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\0potter0\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | loss=0.0886 | test_acc=0.9804\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader):\n",
    "    model.eval()\n",
    "    all_true, all_pred = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_pred.append(pred)\n",
    "        all_true.append(yb.numpy())\n",
    "    return np.concatenate(all_true), np.concatenate(all_pred)\n",
    "\n",
    "EPOCHS = 3  # при GPU можно поставить 5-10\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = train_one_epoch(cnn, train_loader)\n",
    "    y_true_cnn, y_pred_cnn = eval_model(cnn, test_loader)\n",
    "    acc = accuracy_score(y_true_cnn, y_pred_cnn)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | loss={loss:.4f} | test_acc={acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acf1fd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (LeNet) metrics:\n",
      "Accuracy: 0.9804\n",
      "Precision_macro: 0.9803587879014053\n",
      "Recall_macro: 0.9803294113897509\n",
      "F1_macro: 0.9803151105883228\n"
     ]
    }
   ],
   "source": [
    "acc_cnn = accuracy_score(y_true_cnn, y_pred_cnn)\n",
    "prec_cnn, rec_cnn, f1_cnn, _ = precision_recall_fscore_support(y_true_cnn, y_pred_cnn, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"CNN (LeNet) metrics:\")\n",
    "print(\"Accuracy:\", acc_cnn)\n",
    "print(\"Precision_macro:\", prec_cnn)\n",
    "print(\"Recall_macro:\", rec_cnn)\n",
    "print(\"F1_macro:\", f1_cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e172877",
   "metadata": {},
   "source": [
    "### Заключение по сводным метрикам классификации\n",
    "\n",
    "Сводные метрики классификации показывают, что сверточная нейронная сеть LeNet демонстрирует высокое и сбалансированное качество распознавания по всем классам. Значение accuracy составляет около 0.980, при этом macro- и weighted-усреднённые значения precision, recall и F1-мера находятся на сопоставимом уровне.\n",
    "\n",
    "Это свидетельствует о том, что модель одинаково хорошо справляется с классификацией всех цифр и не демонстрирует выраженного перекоса в сторону отдельных классов. Высокие значения weighted average подтверждают, что модель корректно работает с учётом распределения классов в тестовой выборке.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62c77fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (CNN):\n",
      " [[ 971    0    0    0    0    0    3    2    3    1]\n",
      " [   0 1124    2    1    0    0    2    1    5    0]\n",
      " [   5    2 1005    2    2    0    2    7    7    0]\n",
      " [   2    0    3  975    1   10    0    8    7    4]\n",
      " [   0    0    3    0  960    0    4    2    1   12]\n",
      " [   1    0    0    5    1  873    5    1    3    3]\n",
      " [   4    3    0    1    1    1  946    0    2    0]\n",
      " [   1    3    8    0    1    0    0 1009    1    5]\n",
      " [   3    0    0    2    2    1    4    5  955    2]\n",
      " [   2    4    0    1    6    3    0    6    1  986]]\n",
      "\n",
      "Classification report (CNN):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9818    0.9908    0.9863       980\n",
      "           1     0.9894    0.9903    0.9899      1135\n",
      "           2     0.9843    0.9738    0.9791      1032\n",
      "           3     0.9878    0.9653    0.9765      1010\n",
      "           4     0.9856    0.9776    0.9816       982\n",
      "           5     0.9831    0.9787    0.9809       892\n",
      "           6     0.9793    0.9875    0.9834       958\n",
      "           7     0.9693    0.9815    0.9754      1028\n",
      "           8     0.9695    0.9805    0.9750       974\n",
      "           9     0.9733    0.9772    0.9753      1009\n",
      "\n",
      "    accuracy                         0.9804     10000\n",
      "   macro avg     0.9804    0.9803    0.9803     10000\n",
      "weighted avg     0.9805    0.9804    0.9804     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm_cnn = confusion_matrix(y_true_cnn, y_pred_cnn)\n",
    "print(\"Confusion matrix (CNN):\\n\", cm_cnn)\n",
    "\n",
    "print(\"\\nClassification report (CNN):\")\n",
    "print(classification_report(y_true_cnn, y_pred_cnn, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d440a3",
   "metadata": {},
   "source": [
    "## 4) Сравнение результатов и выводы\n",
    "\n",
    "### Почему CNN обычно лучше на MNIST\n",
    "- MLP работает с вектором из 784 признаков и **не учитывает геометрию** изображения.\n",
    "- CNN (LeNet) использует **свёртки**, которые извлекают локальные признаки (штрихи, углы, контуры), и pooling, повышающий устойчивость к небольшим сдвигам.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff70b672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP_sklearn': {'accuracy': 0.9759,\n",
       "  'precision_macro': 0.9757746349708085,\n",
       "  'recall_macro': 0.9756299532723215,\n",
       "  'f1_macro': 0.9756850122423062},\n",
       " 'CNN_LeNet': {'accuracy': 0.9804,\n",
       "  'precision_macro': 0.9803587879014053,\n",
       "  'recall_macro': 0.9803294113897509,\n",
       "  'f1_macro': 0.9803151105883228}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {\n",
    "    \"MLP_sklearn\": {\"accuracy\": float(acc_mlp), \"precision_macro\": float(prec_mlp), \"recall_macro\": float(rec_mlp), \"f1_macro\": float(f1_mlp)},\n",
    "    \"CNN_LeNet\":   {\"accuracy\": float(acc_cnn), \"precision_macro\": float(prec_cnn), \"recall_macro\": float(rec_cnn), \"f1_macro\": float(f1_cnn)},\n",
    "}\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a388e60",
   "metadata": {},
   "source": [
    "### Заключение по сравнению моделей MLP и CNN\n",
    "\n",
    "Итоговое сравнение метрик показывает, что обе модели демонстрируют высокое качество классификации, однако сверточная нейронная сеть LeNet превосходит MLP по всем основным показателям.\n",
    "\n",
    "Модель MLP (scikit-learn) достигла accuracy ≈ 0.976, тогда как CNN (LeNet) показала accuracy ≈ 0.980. Аналогичное преимущество CNN наблюдается и по macro-усреднённым значениям precision, recall и F1-меры.\n",
    "\n",
    "Несмотря на относительно небольшую разницу в абсолютных значениях метрик, преимущество CNN является устойчивым и объясняется архитектурными особенностями модели, ориентированной на работу с изображениями.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7f206",
   "metadata": {},
   "source": [
    "## Итоговое заключение\n",
    "\n",
    "В рамках лабораторной работы была решена задача классификации рукописных цифр датасета MNIST с использованием двух подходов: многослойного перцептрона из библиотеки scikit-learn и сверточной нейронной сети типа LeNet, реализованной на базе PyTorch.\n",
    "\n",
    "Экспериментальные результаты показали, что MLP обеспечивает высокий базовый уровень качества классификации (accuracy ≈ 0.976) и может эффективно применяться для задач многоклассовой классификации при использовании векторного представления данных. Однако данный подход не учитывает пространственную структуру изображений, что ограничивает его выразительную способность.\n",
    "\n",
    "Сверточная нейронная сеть LeNet продемонстрировала более высокие значения всех метрик качества (accuracy ≈ 0.980, более высокие precision, recall и F1-мера). Это преимущество обусловлено использованием сверточных слоев, которые извлекают локальные признаки изображений, такие как контуры и штрихи, а также операций pooling, повышающих устойчивость модели к вариациям написания цифр.\n",
    "\n",
    "Даже при небольшом числе эпох обучения CNN показала стабильную сходимость и лучшую обобщающую способность по сравнению с MLP. Полученные результаты подтверждают, что для задач распознавания изображений сверточные нейронные сети являются более подходящим и эффективным инструментом по сравнению с классическими полносвязными нейронными сетями.\n",
    "\n",
    "Таким образом, цель лабораторной работы была достигнута, а результаты эксперимента наглядно демонстрируют влияние архитектуры нейронной сети на качество решения задачи классификации.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
